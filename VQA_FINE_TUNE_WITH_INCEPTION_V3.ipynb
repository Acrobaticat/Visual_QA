{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.merge import Concatenate\n",
    "import keras\n",
    "from utils.vqa_tools.PythonHelperTools.vqaTools.vqa import VQA\n",
    "\n",
    "# Set VQA dataset files and parameters\n",
    "# dataDir='/Volumes/Data/vqa_data'\n",
    "dataDir='vqa_data'\n",
    "taskType='OpenEnded'\n",
    "dataType='mscoco' # 'mscoco' for real and 'abstract_v002' for abstract\n",
    "dataSubType='train2014'\n",
    "annFile='%s/Annotations/v2_%s_%s_annotations.json'%(dataDir, dataType, dataSubType)\n",
    "quesFile='%s/Questions/v2_%s_%s_%s_questions.json'%(dataDir, taskType, dataType, dataSubType)\n",
    "imgDir = '%s/Images/%s/' %(dataDir, dataSubType)\n",
    "\n",
    "def get_Inception_v3_pre_trained_model():\n",
    "    \n",
    "#     vision_model = Sequential()\n",
    "    print('{} - Starting to get base inception v3 model'.format(datetime.now()))\n",
    "    inception_base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "    vision_model = Flatten()(inception_base_model.output)\n",
    "#     x = inception_base_model.output\n",
    "#     inception_base_model = GlobalAveragePooling2D()(x)\n",
    "#     x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "\n",
    "#     print('{} - Adding base model'.format(datetime.now()))\n",
    "#     vision_model.add(inception_base_model)\n",
    "    \n",
    "    image_input = Input(shape=(224, 224, 3))\n",
    "    encoded_image = vision_model(image_input)\n",
    "#     encoded_image = inception_base_model(image_input)\n",
    "    \n",
    "    print('{} - Processing embedded model'.format(datetime.now()))\n",
    "    question_input = Input(shape=(100,), dtype='int32')\n",
    "    embedded_question = Embedding(input_dim=10000, output_dim=512, input_length=30)(question_input)\n",
    "    encoded_question = LSTM(256)(embedded_question)\n",
    "    \n",
    "    # Let's concatenate the question vector and the image vector:\n",
    "#     merged = keras.layers.concatenate([encoded_question, encoded_image])\n",
    "    print('{} - Merging model'.format(datetime.now()))\n",
    "    merged = keras.layers.concatenate([encoded_question, encoded_image])\n",
    "\n",
    "    # And let's train a logistic regression over 1000 words on top:\n",
    "    output = Dense(1000, activation='softmax')(merged)\n",
    "\n",
    "    # This is our final model:\n",
    "    print('{} - final model'.format(datetime.now()))\n",
    "    vqa_model = Model(inputs=[image_input, question_input], outputs=output)\n",
    "    \n",
    "    return vqa_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-04 02:33:57.278968 - Loading VQA dataset\n",
      "loading VQA annotations and questions into memory...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../vqa_data/Annotations/v2_mscoco_train2014_annotations.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-55714ffc50a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} - Loading VQA dataset'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvqa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVQA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquesFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mannIds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvqa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetQuesIds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquesTypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'how many'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0manns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvqa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadQA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannIds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(\"answer from anns: \", anns[0]['answers'][0]['answer'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/Visual_QA/utils/vqa_tools/PythonHelperTools/vqaTools/vqa.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, annotation_file, question_file)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading VQA annotations and questions into memory...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mtime_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mquestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../vqa_data/Annotations/v2_mscoco_train2014_annotations.json'"
     ]
    }
   ],
   "source": [
    "print('{} - Loading VQA dataset'.format(datetime.now()))\n",
    "vqa = VQA(annFile, quesFile)\n",
    "annIds = vqa.getQuesIds(quesTypes='how many');\n",
    "anns = vqa.loadQA(annIds)\n",
    "# print(\"answer from anns: \", anns[0]['answers'][0]['answer'])\n",
    "print(\"answer(random) from anns: \", random.choice(anns[0]['answers'])['answer'])\n",
    "# print(\"anser from qa: \", vqa.qa[393227002]['answers'][0]['answer'])\n",
    "print(\"querstoin from qqa: \", vqa.qqa[anns[0]['question_id']]['question'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = []\n",
    "question_list = []\n",
    "answer_list = []\n",
    "\n",
    "\n",
    "\n",
    "question_list.append(vqa.qqa[anns[0]['question_id']]['question'])\n",
    "answer_list.append(random.choice(anns[0]['answers'])['answer']) \n",
    "\n",
    "imgId = anns[0]['image_id']\n",
    "imgFilename = 'COCO_' + dataSubType + '_'+ str(imgId).zfill(12) + '.jpg'\n",
    "\n",
    "\n",
    "\n",
    "file = imgDir + imgFilename\n",
    "if os.path.isfile(file):\n",
    "#     im = cv2.resize(cv2.imread(file), (299, 299))\n",
    "#     im = im.astype(np.float32, copy=False)\n",
    "    \n",
    "    im = np.asarray(cv2.imread(file))[:,:,::-1]\n",
    "#     im = central_crop(im, 0.875)\n",
    "    im = cv2.resize(im, (299, 299))\n",
    "#     im = im.reshape(-1,299,299,3)\n",
    "    \n",
    "#     im = io.imread(file)\n",
    "    \n",
    "    print(im.shape)\n",
    "    plt.imshow(im)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    image_list.append(im)\n",
    "\n",
    "# if os.path.isfile(imgDir + imgFilename):\n",
    "#     I = io.imread(imgDir + imgFilename)\n",
    "#     plt.imshow(I)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Loading model .......\n",
      "2017-08-04 02:33:57.537100 - Starting to get base inception v3 model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "verbose = 1\n",
    "if verbose: print(\"\\n\\n\\nLoading model .......\")\n",
    "model = get_Inception_v3_pre_trained_model()\n",
    "\n",
    "# Training\n",
    "epoch = 1\n",
    "batch_size = 1\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "if verbose: print(\"\\n\\n\\nTraining model .......\")\n",
    "train_history = model.fit(x=[image_list, question_list], y=answer_list, \n",
    "                              epochs=epoch, batch_size=batch_size, verbose=2)\n",
    "if verbose: print(\"\\n\\n\\nDone.......\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
